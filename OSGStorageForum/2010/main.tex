% vim: set tw=0:
\documentclass{beamer}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{pdfborder={0 0 0 0}}

% Reasonable themes:
% Antibes Bergen Berkeley Berlin Frankfurt Goettingen Ilmenau Luebeck Malmoe
% Montpellier PaloAlto Rochester Singapore Szeged Warsaw bars boxes
% compatibility default lined plain shadow sidebar split tree
% And these ones include the author's name on every slide:
% Berkeley

% Declare themes.
\mode<presentation>
\usetheme{UWHEP}

% Personal macros.
\newcommand{\email}[1]{{\texttt #1}}
\newcommand{\newframe}[1]{\section{#1}
    \frametitle{\sc{#1}}}
\newcommand{\subframe}[1]{\subsection{#1}
    \frametitle{\sc{#1}}}
\newcommand{\supers}[1]{\ensuremath{^\textrm{#1}}}
\newcommand{\subs}[1]{\ensuremath{_\textrm{#1}}}
\newcommand{\ca}{\ensuremath{\sim}}
\renewcommand{\email}[1]{\href{mailto:#1}{\nolinkurl{#1}}}

% Author information.
\title{Storage at UW-Madison CMS Tier-2}
\author[Maier]{
    Will Maier \\ 
    {\tt wcmaier@hep.wisc.edu}}
\institute[Wisconsin]{University of Wisconsin - High Energy Physics}
\date[2010.09.22]{OSG Storage Forum, U. Chicago}
\logo{\includegraphics[height=0.6cm]{../../Graphics/USCMS_logo.png}\hspace{.1cm}\includegraphics[height=0.75cm]{../../Graphics/UW_logo.png}}

\begin{document}

% http://indico.fnal.gov/conferenceTimeTable.py?confId=3377
% 15 min
% hardware/layout
% software
% monitoring/management
% evaluation
% plans

\begin{frame}
    \titlepage
\end{frame}

\section{Overview}
\begin{frame}
    \tableofcontents
\end{frame}

\section{Facilities}
\subsection{Configuration}
\begin{frame}
\begin{itemize}
	\item dCache 1.9.5-8 (Golden Release)
	\begin{itemize}
		\item Configuration available online~\footnote{\url{http://code.hep.wisc.edu/dcache-cftree}}
	\end{itemize}
	\item dCache SRM/GridFTP/dcap doors
	\item Xrootd Demonstrator instance
	\item Scientific Linux 5.3
\end{itemize}
\end{frame}

\subsection{Core}
\begin{frame}
\begin{itemize}
	\item Four core dCache machines: PNFS, admin/misc, doors, hotspare
	\item 1 Gbps connections, SATA disks, 2x2 2.0 GHz Opterons, 16 GB RAM
	\item PNFS: RAID10 (10kRPM disks), 2x4 3.0 GHz Xeons
	\begin{itemize}
		\item Current configuration \ca{}12 months old
		\item Nearly saturated (again)\ldots{}need more IO
	\end{itemize}
	\item admin, doors, hotspare: commodity, nothing special
	\begin{itemize}
		\item No observed scaling issues with shared SRM/dcap door
		\item \ca{}32 GridFTP doors live on data nodes (easy to add more)
	\end{itemize}
	% plot requests/time? files?
\end{itemize}
\end{frame}

\subsection{Cluster}
\begin{frame}
\begin{itemize}
	\item \ca{}50\% dedicated, \ca{}50\% shared with compute resources
	\item Dedicated nodes: mix of RAID6, RAID0 with 1 - 2 TB SATA disks
	\item Shared nodes: no RAID, .5 - 2 TB SATA disks
	\begin{itemize}
		\item Co-located with 4 - 16 slots and 2 - 48 GB RAM for computing
	\end{itemize}
	\item \ca{}950 raw TB, \ca{}450 writable TB, 908(ish) pools, 210 nodes
	\begin{itemize}
		\item One pool per disk, mostly
		\item 100\% increase over 2009
	\end{itemize}
	\item Redundant 2x10 Gbps uplink (and several stacks of Cisco 3750s)
	\begin{itemize}
		\item Managed by campus
		\item Eliminated a few Etherchannels connecting racks
		\item Upgraded rest to 10 Gbps bundles
	\end{itemize}
	\item Burn in new hardware using stressapptest~\footnote{~\url{http://code.google.com/p/stressapptest/}}
	% XXX: plot: TB/time
\end{itemize}
\end{frame}

\begin{frame}
\begin{figure}
	% http://myosg.grid.iu.edu/wizardse/index?datasource=se&summary_attrs_showservice=on&summary_attrs_showrsvstatus=on&summary_attrs_showfqdn=on&gip_status_attrs_showtestresults=on&gip_status_attrs_showfqdn=on&account_type=daily_hours_byusername&ce_account_type=gip_vo&se_account_type=se_space&start_type=specific&start_date=09/22/2009&end_type=now&end_date=04/15/2009&facility_10017=on&site_10026=on&r=on&r_24=on&r_27=on&gridtype=on&gridtype_1=on&voown_3=on&active_value=1&disable_value=1
    \includegraphics[width=.8\textwidth]{Graphics/sespace.png}
    \caption{Total dCache (TB), 2009-2010}
\end{figure}
\end{frame}

\begin{frame}
\begin{figure}
	% http://www.hep.wisc.edu/~ajit/phedex_prod_xfer_rates_Dec2009.png
	\includegraphics[width=\textwidth]{Graphics/phedex_prod_xfer_rates_Dec2009.png}
	\caption{PhEDEx peak transfers, late 2009}
\end{figure}
\end{frame}

\section{Management}
\subsection{Administration}
\begin{frame}
\begin{itemize}
	\item Install nodes with kickstart, manage live configuration with Cfengine
	\item Management scripts written with dcache-tools~\footnote{\url{http://code.hep.wisc.edu/dcache-tools}}, based on CLI~\footnote{\url{http://packages.python.org/pyCLI}}
	\begin{itemize}
		\item {\tt dcache\_absent}: List absent and offline pools
		\item {\tt dcache\_billingrep}: Watch billing log and replicate new files
		\item {\tt dcache\_clean}: Remove invalid/unlinked files directly from pools
		\item {\tt dcache\_df}: {\tt df(1)}-like overview of cluster storage
		\item etc\ldots{}
	\end{itemize}
	\item Less web scraping, more dCache info provider
	\begin{itemize}
		\item Previously, monitors/checks polled dCache web pages and scraped HTML
		\item Info provider allows simple XML parsing (and exposes lots of detail)
		\item Can request subsets of data (ie for {\tt dcache\_df})
	\end{itemize}
\end{itemize}
\end{frame}

\subsection{Alerts and Trends} % nagios
\begin{frame}
\begin{itemize}
	\item Nagios checks for all servers in both the core and cluster
	\item Some Nagios protocol checks (GridFTP, SSH admin interface)
	\item Functional tests via cron jobs (SRM/dcap/GridFTP read/write)
	\item External monitoring (CMS SAM, JobRobot, Dashboard, SiteView, PhEDEx; OSG RSV, MyOSG, Gratia; \ldots{})
	\item Aggregate local monitoring in tsar~\footnote{\url{http://code.hep.wisc.edu/tsar}}
	\begin{itemize}
		\item Ingest time series data from simple collectors
		\item Supports HTTP, other submission/query protocols to come
		\item Data in memory for fast writes/reads (Redis~\footnote{\url{http://redis.io}} backend)
		\item JavaScript, Python clients for visualization~\footnote{\url{http://tsar.hep.wisc.edu/plots/dcache}}
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{figure}
	% http://tsar.hep.wisc.edu/plots/dcache
	\includegraphics[width=.6\textwidth]{Graphics/tsar.png}
	\caption{24 hours of dCache latency checks in tsar, 2010.09.22}
\end{figure}
\end{frame}

\section{Evaluation}
\subsection{What Works}
\begin{frame}
\begin{itemize}
	\item Close network collaboration with University
	\item Co-locating storage and compute services on commodity hardware
	\item dCache (stability, throughput, OSG/FNAL support)
	\item Increasingly centralized monitoring and reporting
\end{itemize}
\end{frame}

\subsection{What Doesn't}
\begin{frame}
\begin{itemize}
	\item Disks
	\item Monitoring fragmentation
	\begin{itemize}
		\item Takes Firefox nearly two minutes to load them all on a netbook
	\end{itemize}
	\item dCache (hotspots, replication, commodity data servers)
	\begin{itemize}
		\item Model mismatch: small precious storage with large tape backend vs precious storage (no tape)
		\item We've written thousands of lines of code to make running dCache at our T2 livable
	\end{itemize}
\end{itemize}
\end{frame}

\section{Plans}
\begin{frame}
\begin{itemize}
	\item Centralize dCache logging
	\item Scale tsar
	\item Transition to alerting based on trends (instead of binary checks)
	\item Explore HDFS w/ xrootd frontend (to support jobs on local opportunistic resources)
	\item PNFS $\rightarrow$ Chimera? dCache $\rightarrow$ HDFS?
	\item Cfengine 2 $\rightarrow$ Cfengine 3/Puppet/Chef/\ldots{}?
	\item Expand use of RPMs (for improved atomicity?)
\end{itemize}
\end{frame}

\end{document}
